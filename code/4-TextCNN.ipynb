{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir923/wujiaming/anaconda3/envs/tf2/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from load_data import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "def use_gpu_polite(using_rate=0.6):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = using_rate\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickle data from ../data/imdb_data_3col.pkl\n",
      "Original 134957 words in vocabulary.\n",
      "After truncated low frequent word:\n",
      "words num: 40000/134957; words freq: 0.981\n",
      "Words exit in w2v file: 39210/40004, rate: 98.015198%\n",
      "Shape of weight matrix: (40006, 50)\n",
      "Train data shape: (25000, 500) label length: 25000\n",
      "Test data shape: (25000, 500) label length: 25000\n",
      "dict_keys(['data', 'data_len', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# 导入数据，train data 是一个字典， ebd embdding 是词向量矩阵，作为embedding层的初始参数\n",
    "train_data, test_data, ebd_weights = load_imdb_data()\n",
    "print(train_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN模型，完全细节到每个参数设计，注意各个参数维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN():\n",
    "    def __init__(self, seq_length, class_num, ebd_weights, filter_num, filter_sizes = [2, 3, 4]):\n",
    "        seed_num = 7  # 参数初始化种子\n",
    "\n",
    "        self.seq_input = tf.placeholder(dtype=tf.int32, shape=[None, seq_length],\n",
    "                                        name='sequence_input')\n",
    "        self.sparse_label_input = tf.placeholder(dtype=tf.int32, shape=[None],\n",
    "                                                 name='sparse_label')\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.global_step_op = tf.assign(self.global_step, self.global_step+1)\n",
    "\n",
    "        embedding_dim = ebd_weights.shape[1]\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.W = tf.Variable(initial_value=ebd_weights, name='W')\n",
    "            self.embedding_layer = tf.nn.embedding_lookup(self.W, self.seq_input)\n",
    "            # batch * seq_len * emb_dim * in_channel(1)\n",
    "            self.embedding_layer_expand = tf.expand_dims(self.embedding_layer, axis=-1)\n",
    "\n",
    "        pool_layers = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope('conv-maxpool-%s' % i):\n",
    "                # flter: seq_size * emb_dim * in_channel * out_channelout_channel\n",
    "                filter_shape = [filter_size, embedding_dim, 1, filter_num]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1,\n",
    "                                                         seed=seed_num), name='W')\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[filter_num]), name='b')\n",
    "                conv = tf.nn.conv2d(self.embedding_layer_expand, W,\n",
    "                                         strides=[1, 1, 1, 1], padding='VALID', name='conv')\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "\n",
    "                pooling = tf.nn.max_pool(h, ksize=[1, seq_length-filter_size+1, 1, 1],\n",
    "                                        strides=[1, 1, 1, 1], padding='VALID', name='pool')\n",
    "                pool_layers.append(pooling)\n",
    "\n",
    "        # batch * 1 * 1 * all --> batch * all\n",
    "        all_dim = len(filter_sizes) * filter_num\n",
    "        self.pool_flatten = tf.reshape(tf.concat(pool_layers, axis=-1), shape=[-1, all_dim])\n",
    "\n",
    "        with tf.name_scope('output'):\n",
    "            W = tf.Variable(tf.truncated_normal([all_dim, class_num], seed=seed_num, name='W'))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[class_num]), name='b')\n",
    "            self.logits = tf.nn.xw_plus_b(self.pool_flatten, W, b, name='logits')\n",
    "            self.prediction = tf.argmax(self.logits, axis=-1, output_type=tf.int32, name='prediction')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.sparse_label_input, logits=self.logits)\n",
    "            self.loss_sum = tf.reduce_sum(losses)\n",
    "            self.loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predction = tf.equal(self.prediction, self.sparse_label_input)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predction, tf.float16), name='correct_num')\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predction, tf.float16), name='accuracy')\n",
    "        \n",
    "        self.learning_rate = 1e-3\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        gs_vs = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(gs_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练，观察训练集和测试集上的loss 和 acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train loss 4.5819, acc 0.6213; test loss 5.1673, acc 0.5993\n",
      "Epoch 20, train loss 2.6118, acc 0.7114; test loss 3.4717, acc 0.6624\n",
      "Epoch 30, train loss 1.6697, acc 0.7718; test loss 2.7233, acc 0.6979\n",
      "Epoch 40, train loss 1.0503, acc 0.8226; test loss 2.2395, acc 0.7231\n",
      "Epoch 50, train loss 0.6041, acc 0.8753; test loss 1.8714, acc 0.7443\n",
      "Epoch 60, train loss 0.3017, acc 0.9246; test loss 1.5883, acc 0.7629\n",
      "Epoch 70, train loss 0.1162, acc 0.9681; test loss 1.3582, acc 0.7820\n",
      "Epoch 80, train loss 0.0365, acc 0.9925; test loss 1.2035, acc 0.8010\n",
      "Epoch 90, train loss 0.0189, acc 0.9960; test loss 1.1804, acc 0.8056\n",
      "Epoch 100, train loss 0.0047, acc 0.9991; test loss 1.1688, acc 0.8024\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = train_data['data'], [1 if i == 'pos' else 0 for i in train_data['label']]\n",
    "test_x, test_y = test_data['data'], [1 if i == 'pos' else 0 for i in test_data['label']]\n",
    "cnn_model = TextCNN(seq_length=train_x.shape[1], class_num=2, ebd_weights=ebd_weights, filter_num=32)\n",
    "batch_size = 1000\n",
    "epoch_max = 100\n",
    "\n",
    "\n",
    "config = use_gpu_polite()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch_now = sess.run(cnn_model.global_step)\n",
    "    \n",
    "    while epoch_now < epoch_max:\n",
    "        # 在训练集上按batch训练完所有，算作一个epoch\n",
    "        batch_num = train_x.shape[0] // batch_size\n",
    "        for i in range(batch_num+1):\n",
    "            s_i = i * batch_size\n",
    "            e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "            if s_i >= e_i:\n",
    "                continue\n",
    "            in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "            feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "            sess.run(cnn_model.train_op, feed_dict)\n",
    "\n",
    "        epoch_now = sess.run(cnn_model.global_step_op)  # 跑完了一个epoch，epoch+1\n",
    "        \n",
    "        if epoch_now % 10 == 0:  # 每10轮观察一下训练集测试集loss 和 acc\n",
    "            # 训练集总的损失和acc也要分步测，否则内存不够\n",
    "            batch_num = train_x.shape[0] // batch_size\n",
    "            train_total_loss = 0\n",
    "            train_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                train_loss_one, train_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                train_total_loss += train_loss_one\n",
    "                train_total_correct += train_correct_one\n",
    "            train_loss = train_total_loss / train_x.shape[0]\n",
    "            train_acc = train_total_correct / train_x.shape[0]\n",
    "\n",
    "            # 测试集的损失和acc\n",
    "            batch_num = test_x.shape[0] // batch_size\n",
    "            test_total_loss = 0\n",
    "            test_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, test_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = test_x[s_i: e_i, :], test_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                test_loss_one, test_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                test_total_loss += test_loss_one\n",
    "                test_total_correct += test_correct_one\n",
    "            test_loss = test_total_loss / test_x.shape[0]\n",
    "            test_acc = test_total_correct / test_x.shape[0]\n",
    "\n",
    "            print('Epoch %d, train loss %.4f, acc %.4f; test loss %.4f, acc %.4f' % \n",
    "                  (epoch_now, train_loss, train_acc, test_loss, test_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

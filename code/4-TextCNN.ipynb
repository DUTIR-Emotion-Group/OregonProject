{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir923/wujiaming/anaconda3/envs/tf2/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from load_data import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' #使用 GPU 0\n",
    "def use_gpu_polite(using_rate=0.6):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = using_rate\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickle data from ../data/imdb_data_3col.pkl\n",
      "Original 134957 words in vocabulary.\n",
      "After truncated low frequent word:\n",
      "words num: 40000/134957; words freq: 0.981\n",
      "Words exit in w2v file: 39210/40004, rate: 98.015198%\n",
      "Shape of weight matrix: (40006, 50)\n",
      "Train data shape: (25000, 500) label length: 25000\n",
      "Test data shape: (25000, 500) label length: 25000\n",
      "dict_keys(['data', 'data_len', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# 导入数据，train data 是一个字典， ebd embdding 是词向量矩阵，作为embedding层的初始参数\n",
    "train_data, test_data, ebd_weights = load_imdb_data()\n",
    "print(train_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN模型，完全细节到每个参数设计，注意各个参数维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN():\n",
    "    def __init__(self, seq_length, class_num, ebd_weights, filter_num, filter_sizes = [2, 3, 4]):\n",
    "        seed_num = 7  # 参数初始化种子\n",
    "\n",
    "        self.seq_input = tf.placeholder(dtype=tf.int32, shape=[None, seq_length],\n",
    "                                        name='sequence_input')\n",
    "        self.sparse_label_input = tf.placeholder(dtype=tf.int32, shape=[None],\n",
    "                                                 name='sparse_label')\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.global_step_op = tf.assign(self.global_step, self.global_step+1)\n",
    "\n",
    "        embedding_dim = ebd_weights.shape[1]\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.W = tf.Variable(initial_value=ebd_weights, name='W')\n",
    "            self.embedding_layer = tf.nn.embedding_lookup(self.W, self.seq_input)\n",
    "            # batch * seq_len * emb_dim * in_channel(1)\n",
    "            self.embedding_layer_expand = tf.expand_dims(self.embedding_layer, axis=-1)\n",
    "\n",
    "        pool_layers = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope('conv-maxpool-%s' % i):\n",
    "                # flter: seq_size * emb_dim * in_channel * out_channelout_channel\n",
    "                filter_shape = [filter_size, embedding_dim, 1, filter_num]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1,\n",
    "                                                         seed=seed_num), name='W')\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[filter_num]), name='b')\n",
    "                conv = tf.nn.conv2d(self.embedding_layer_expand, W,\n",
    "                                         strides=[1, 1, 1, 1], padding='VALID', name='conv')\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "\n",
    "                pooling = tf.nn.max_pool(h, ksize=[1, seq_length-filter_size+1, 1, 1],\n",
    "                                        strides=[1, 1, 1, 1], padding='VALID', name='pool')\n",
    "                pool_layers.append(pooling)\n",
    "\n",
    "        # batch * 1 * 1 * all --> batch * all\n",
    "        all_dim = len(filter_sizes) * filter_num\n",
    "        self.pool_flatten = tf.reshape(tf.concat(pool_layers, axis=-1), shape=[-1, all_dim])\n",
    "\n",
    "        with tf.name_scope('output'):\n",
    "            W = tf.Variable(tf.truncated_normal([all_dim, class_num], seed=seed_num, name='W'))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[class_num]), name='b')\n",
    "            self.logits = tf.nn.xw_plus_b(self.pool_flatten, W, b, name='logits')\n",
    "            self.prediction = tf.argmax(self.logits, axis=-1, output_type=tf.int32, name='prediction')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.sparse_label_input, logits=self.logits)\n",
    "            self.loss_sum = tf.reduce_sum(losses)\n",
    "            self.loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predction = tf.equal(self.prediction, self.sparse_label_input)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predction, tf.float16), name='correct_num')\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predction, tf.float16), name='accuracy')\n",
    "        \n",
    "        self.learning_rate = 1e-3\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        gs_vs = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(gs_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练，观察训练集和测试集上的loss 和 acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train loss 4.5929, acc 0.6203; test loss 5.1768, acc 0.5987\n",
      "[[5.095503   0.62826145 3.9841487  5.5019765  2.220422   4.743357\n",
      "  5.8036704  3.8505344  4.7289195  4.699203   4.436813   4.1560545\n",
      "  5.133413   3.617564   4.3160853  5.2199793  3.594837   3.6712592\n",
      "  4.0461044  4.77945    3.2232623  2.431066   6.8121934  6.1613665\n",
      "  3.3625274  3.9509478  3.7393603  4.708541   6.1360946  4.6546593\n",
      "  4.4054646  3.2134683  5.3733535  5.9744163  5.67858    7.0276327\n",
      "  4.220739   5.2743993  5.61334    5.665866   4.211244   5.8552175\n",
      "  7.701244   4.1235967  7.8108835  4.97903    4.7353644  7.0625434\n",
      "  4.47971    5.093168   4.5620117  6.091928   5.3831916  0.49923554\n",
      "  8.651499   6.2799897  1.7130096  5.8069344  2.5869129  6.419062\n",
      "  5.96157    4.238068   3.4507482  3.3257768  5.517814   5.8900013\n",
      "  5.8953958  6.2903595  5.375534   6.1365404  5.9723315  4.97507\n",
      "  6.4751463  5.5900445  6.282504   4.571477   7.2361274  5.486262\n",
      "  6.6281266  6.0197763  5.3447547  5.01732    4.02223    5.8766747\n",
      "  5.372338   4.437678   8.020511   3.8339603  6.462383   5.357779\n",
      "  6.038814   5.4217625  6.2347617  4.6334596  6.0417814  6.6923547 ]] \n",
      " [[2.049914  2.3981383]]\n",
      "Epoch 20, train loss 2.6048, acc 0.7109; test loss 3.4623, acc 0.6627\n",
      "[[5.0374537  0.20169006 4.1524854  5.637159   2.173282   4.338863\n",
      "  5.747033   3.850494   4.6497827  4.6945133  4.391072   4.157037\n",
      "  4.9185433  3.4562824  3.7654326  4.643297   3.8703434  3.59493\n",
      "  4.0091805  4.768045   3.1181848  2.5812573  6.111024   6.0998063\n",
      "  3.2549725  3.9496138  3.586133   3.8339515  6.046499   4.4779253\n",
      "  4.172714   3.1812992  5.1573124  6.42609    5.8029613  6.8456593\n",
      "  4.0448747  4.245986   5.6563473  5.510957   4.0545382  5.8071084\n",
      "  7.4177976  4.100821   7.4779162  4.9071364  4.6460733  7.1730685\n",
      "  4.1387324  4.9958735  4.504393   5.9119687  5.145508   0.17729457\n",
      "  8.559956   6.0859575  1.1877984  5.6078043  2.331784   6.2725616\n",
      "  5.8699994  3.9788022  3.3835542  2.7786837  4.871712   5.6569705\n",
      "  5.6677327  5.7281094  5.544006   5.4724584  6.2652254  4.8872538\n",
      "  6.491814   5.4117637  5.566813   4.0341473  6.535915   5.4647336\n",
      "  6.655461   5.871929   5.7284837  5.125806   3.6731336  5.770234\n",
      "  5.0384254  4.476157   7.83836    2.1668584  6.8125286  5.52698\n",
      "  5.6289406  5.345832   5.9550204  4.533064   5.662689   6.5806146 ]] \n",
      " [[1.6985383 3.4159706]]\n",
      "Epoch 30, train loss 1.6798, acc 0.7702; test loss 2.7341, acc 0.6975\n",
      "[[5.011316   0.08222337 4.496642   5.6403646  2.0370455  4.138485\n",
      "  5.3259225  3.7858014  4.690321   4.685373   4.396319   4.3162065\n",
      "  4.604574   3.3761125  3.43674    4.2987323  4.042048   3.5671263\n",
      "  3.9756956  4.831029   3.0556479  2.586855   6.0741305  6.1440997\n",
      "  3.182244   3.9045098  3.4943655  3.4717093  5.9662223  4.2644057\n",
      "  3.9451213  3.2173703  5.264036   6.48947    5.7349143  6.6773148\n",
      "  3.9099197  3.5128312  5.4171505  5.345954   4.0408597  5.738021\n",
      "  6.9096713  4.203217   7.191925   4.755664   4.859199   7.1622014\n",
      "  3.884401   4.9931607  4.400687   5.8998494  5.179246   0.11012352\n",
      "  8.459316   5.868273   0.9718658  5.3798857  2.1089287  6.100333\n",
      "  5.6853523  3.8149726  3.3905542  2.4278445  4.4206705  5.837451\n",
      "  5.456283   5.5223546  5.6191206  5.016776   6.4103785  5.2034187\n",
      "  6.333238   5.224447   5.3205414  3.7064168  6.0631604  5.3936305\n",
      "  6.553752   5.679388   5.931549   5.171916   3.719456   5.8861337\n",
      "  4.697348   4.332664   7.8380756  1.3629771  6.7571015  5.4559913\n",
      "  5.4270053  5.2417164  5.8708324  4.5128713  5.4996567  6.673411  ]] \n",
      " [[1.9509802 4.2777915]]\n",
      "Epoch 40, train loss 1.0573, acc 0.8222; test loss 2.2450, acc 0.7222\n",
      "[[5.028223   0.07306713 4.636962   5.5038376  1.9161987  3.927691\n",
      "  4.931473   3.7334394  4.6726084  4.6639276  4.390222   4.4374504\n",
      "  4.299823   3.2482805  3.4308927  4.0247736  4.2772098  3.5371199\n",
      "  3.9580972  4.8856835  2.9625764  2.5692823  6.1899853  6.227915\n",
      "  3.1218135  3.876945   3.3941603  3.2568548  5.9071755  4.1032405\n",
      "  3.7330194  3.2398527  5.2961726  6.4036565  5.604952   6.7256646\n",
      "  3.802229   3.306096   5.2750483  5.1954913  4.0632324  5.7346435\n",
      "  6.6352253  4.3166156  6.887639   4.6126494  4.8707423  7.1417813\n",
      "  3.6685405  4.9480867  4.2319765  5.909608   5.259002   0.16688843\n",
      "  8.362209   5.7030077  0.7730177  5.4400783  2.0103555  5.9819736\n",
      "  5.6181445  3.6778178  3.375893   2.2317863  3.9107003  5.9335785\n",
      "  5.373196   5.7498994  5.6565914  4.6799555  6.3692985  5.3697195\n",
      "  6.304361   5.197103   5.1187205  3.399954   5.640136   5.243069\n",
      "  6.3913894  5.5121436  6.0379677  5.165332   3.8647957  5.9544263\n",
      "  4.4980717  4.3148794  7.901391   0.8210724  6.679141   5.3879666\n",
      "  5.336563   5.1835628  5.824075   4.564509   5.3958106  6.731105  ]] \n",
      " [[2.3067143 6.0201516]]\n",
      "Epoch 50, train loss 0.6104, acc 0.8745; test loss 1.8776, acc 0.7435\n",
      "[[4.9585824  0.14093412 4.6925282  5.355544   1.8439854  3.8481627\n",
      "  4.601653   3.7462556  4.635421   4.607965   4.360705   4.582568\n",
      "  4.13999    3.1393244  3.649631   3.8817575  4.372137   3.5154965\n",
      "  3.921902   4.924033   2.9070811  2.5391996  6.2123246  6.3430595\n",
      "  3.0965176  3.856919   3.2978349  3.1684642  5.8620877  3.8996983\n",
      "  3.6236098  3.2239633  5.194576   6.2634387  5.5099845  6.796176\n",
      "  3.8016398  3.2167869  5.256018   5.095159   4.0865755  5.7180295\n",
      "  6.6408544  4.433039   6.5507336  4.4431734  4.7636805  7.1083784\n",
      "  3.5261433  4.9209943  4.1307654  5.945671   5.3235455  0.1639103\n",
      "  8.16382    5.5911837  0.6224306  5.6053443  1.9885627  5.8179164\n",
      "  5.528762   3.6217058  3.411577   2.218139   3.764816   5.9491916\n",
      "  5.3363     5.7841377  5.655089   4.5851216  6.252309   5.522526\n",
      "  6.193071   5.189025   5.1743956  3.240109   5.4127164  5.010862\n",
      "  6.29538    5.4290266  6.098037   5.182443   4.019448   5.9744854\n",
      "  4.433184   4.3736835  7.9156976  0.52297723 6.5861983  5.3084855\n",
      "  5.3233786  5.1196465  5.8474197  4.5426683  5.3090982  6.696943  ]] \n",
      " [[1.8635379 7.297759 ]]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = train_data['data'], [1 if i == 'pos' else 0 for i in train_data['label']]\n",
    "test_x, test_y = test_data['data'], [1 if i == 'pos' else 0 for i in test_data['label']]\n",
    "cnn_model = TextCNN(seq_length=train_x.shape[1], class_num=2, ebd_weights=ebd_weights, filter_num=32)\n",
    "batch_size = 1000\n",
    "epoch_max = 50\n",
    "\n",
    "\n",
    "config = use_gpu_polite()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch_now = sess.run(cnn_model.global_step)\n",
    "    \n",
    "    while epoch_now < epoch_max:\n",
    "        # 在训练集上按batch训练完所有，算作一个epoch\n",
    "        batch_num = train_x.shape[0] // batch_size\n",
    "        for i in range(batch_num+1):\n",
    "            s_i = i * batch_size\n",
    "            e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "            if s_i >= e_i:\n",
    "                continue\n",
    "            in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "            feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "            sess.run(cnn_model.train_op, feed_dict)\n",
    "\n",
    "        epoch_now = sess.run(cnn_model.global_step_op)  # 跑完了一个epoch，epoch+1\n",
    "        \n",
    "        if epoch_now % 10 == 0:  # 每10轮观察一下训练集测试集loss 和 acc\n",
    "            # 训练集总的损失和acc也要分步测，否则内存不够\n",
    "            batch_num = train_x.shape[0] // batch_size\n",
    "            train_total_loss = 0\n",
    "            train_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                train_loss_one, train_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                train_total_loss += train_loss_one\n",
    "                train_total_correct += train_correct_one\n",
    "            train_loss = train_total_loss / train_x.shape[0]\n",
    "            train_acc = train_total_correct / train_x.shape[0]\n",
    "\n",
    "            # 测试集的损失和acc\n",
    "            batch_num = test_x.shape[0] // batch_size\n",
    "            test_total_loss = 0\n",
    "            test_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, test_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = test_x[s_i: e_i, :], test_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                test_loss_one, test_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                test_total_loss += test_loss_one\n",
    "                test_total_correct += test_correct_one\n",
    "            test_loss = test_total_loss / test_x.shape[0]\n",
    "            test_acc = test_total_correct / test_x.shape[0]\n",
    "\n",
    "            # 查看一组 中间参数\n",
    "            flatten_param, logits = sess.run([cnn_model.pool_flatten, cnn_model.logits], \n",
    "                                             {cnn_model.seq_input: train_x[0:1, :]})\n",
    "            print('Epoch %d, train loss %.4f, acc %.4f; test loss %.4f, acc %.4f' % \n",
    "                  (epoch_now, train_loss, train_acc, test_loss, test_acc))\n",
    "            print(flatten_param, '\\n', logits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

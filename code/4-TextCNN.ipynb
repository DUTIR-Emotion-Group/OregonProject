{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' #使用 GPU 0\n",
    "def use_gpu_polite(using_rate=0.6):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = using_rate\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickle data from ../data/imdb_data_3col.pkl\n",
      "Original 134957 words in vocabulary.\n",
      "After truncated low frequent word:\n",
      "words num: 40000/134957; words freq: 0.981\n",
      "Words exit in w2v file: 39210/40004, rate: 98.015198%\n",
      "Shape of weight matrix: (40006, 50)\n",
      "Train data shape: (25000, 500) label length: 25000\n",
      "Test data shape: (25000, 500) label length: 25000\n",
      "dict_keys(['data', 'data_len', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# 导入数据，train data 是一个字典， ebd embdding 是词向量矩阵，作为embedding层的初始参数\n",
    "train_data, test_data, ebd_weights = load_imdb_data()\n",
    "print(train_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN模型，完全细节到每个参数设计，注意各个参数维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN():\n",
    "    def __init__(self, seq_length, class_num, ebd_weights, filter_num, filter_sizes = [2, 3, 4]):\n",
    "        seed_num = 7  # 参数初始化种子\n",
    "\n",
    "        self.seq_input = tf.placeholder(dtype=tf.int32, shape=[None, seq_length],\n",
    "                                        name='sequence_input')\n",
    "        self.sparse_label_input = tf.placeholder(dtype=tf.int32, shape=[None],\n",
    "                                                 name='sparse_label')\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.global_step_op = tf.assign(self.global_step, self.global_step+1)\n",
    "\n",
    "        embedding_dim = ebd_weights.shape[1]\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.W = tf.Variable(initial_value=ebd_weights, name='W')\n",
    "            self.embedding_layer = tf.nn.embedding_lookup(self.W, self.seq_input)\n",
    "            # batch * seq_len * emb_dim * in_channel(1)\n",
    "            self.embedding_layer_expand = tf.expand_dims(self.embedding_layer, axis=-1)\n",
    "\n",
    "        pool_layers = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope('conv-maxpool-%s' % i):\n",
    "                # flter: seq_size * emb_dim * in_channel * out_channelout_channel\n",
    "                filter_shape = [filter_size, embedding_dim, 1, filter_num]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1,\n",
    "                                                         seed=seed_num), name='W')\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[filter_num]), name='b')\n",
    "                conv = tf.nn.conv2d(self.embedding_layer_expand, W,\n",
    "                                         strides=[1, 1, 1, 1], padding='VALID', name='conv')\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "\n",
    "                pooling = tf.nn.max_pool(h, ksize=[1, seq_length-filter_size+1, 1, 1],\n",
    "                                        strides=[1, 1, 1, 1], padding='VALID', name='pool')\n",
    "                pool_layers.append(pooling)\n",
    "\n",
    "        # batch * 1 * 1 * all --> batch * all\n",
    "        all_dim = len(filter_sizes) * filter_num\n",
    "        self.pool_flatten = tf.reshape(tf.concat(pool_layers, axis=-1), shape=[-1, all_dim])\n",
    "\n",
    "        with tf.name_scope('output'):\n",
    "            W = tf.Variable(tf.truncated_normal([all_dim, class_num], seed=seed_num, name='W'))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[class_num]), name='b')\n",
    "            self.logits = tf.nn.xw_plus_b(self.pool_flatten, W, b, name='logits')\n",
    "            self.prediction = tf.argmax(self.logits, axis=-1, output_type=tf.int32, name='prediction')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.sparse_label_input, logits=self.logits)\n",
    "            self.loss_sum = tf.reduce_sum(losses)\n",
    "            self.loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predction = tf.equal(self.prediction, self.sparse_label_input)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predction, tf.float16), name='correct_num')\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predction, tf.float16), name='accuracy')\n",
    "        \n",
    "        self.learning_rate = 1e-3\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        gs_vs = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(gs_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练，观察训练集和测试集上的loss 和 acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train loss 4.9552, acc 0.6108; test loss 5.5396, acc 0.5919\n",
      "[[ 7.1498394  0.8788357  8.762747   4.077044   4.4431653  4.083873\n",
      "   4.6829095  4.562131   5.4222827  3.3322916  3.2047102  5.530068\n",
      "   4.7164817  4.5627437  3.3464746  3.932323   5.089951   5.8267713\n",
      "   5.4085298  6.7076097  4.6443925  3.4318924  5.177335   4.400442\n",
      "   2.846754   3.0878446  2.151237   4.433712   6.0741196  4.3684387\n",
      "   4.1369505  5.1508846  5.5562134  8.116441  11.450643   8.524153\n",
      "   7.0359306  5.775443   4.155632   6.267608   5.3239923  4.902242\n",
      "   4.4911404  6.919862   6.0777497  7.3242097  3.6473885  3.5596538\n",
      "   4.693835   5.724621   6.264509   6.648917   5.693319   1.3753245\n",
      "   4.5376434  3.9895556  3.444441   4.6563606  2.8178558  6.812421\n",
      "   6.188737   5.5160513  3.4409053  3.0542064  6.5667367  7.965968\n",
      "   7.4415135  6.770208   6.1469736  5.67134    6.4630575  6.8042417\n",
      "   6.984965   6.3311715  4.781059   6.778341   9.2300825  9.070978\n",
      "   3.5459645  5.625981   5.7714396  6.242711   9.199201   6.24925\n",
      "   6.6427956  4.6118593  6.706541   2.1066883  4.022748   4.756533\n",
      "   5.1126146  6.496985   5.8661017  5.2950907  5.9583545  4.80845  ]] \n",
      " [[11.956393  9.147152]]\n",
      "Epoch 20, train loss 2.8221, acc 0.7004; test loss 3.6875, acc 0.6555\n",
      "[[ 6.8051805   0.6318595   7.964375    4.0986056   4.035598    4.103732\n",
      "   4.751911    4.43999     5.4065723   3.0621996   3.1320348   5.146291\n",
      "   4.484409    4.45022     3.2621858   3.6782725   4.827565    5.314139\n",
      "   5.2796507   6.5199943   4.631897    3.3137178   5.286385    4.192841\n",
      "   3.028013    3.1294918   2.1986704   3.8501134   5.6220226   4.308268\n",
      "   3.5482686   5.0558224   5.811303    7.590966   11.459156    8.096773\n",
      "   6.483784    5.4430394   4.03784     6.108888    4.9045324   4.5118175\n",
      "   4.569797    6.506461    5.7315345   7.22031     3.3125782   3.25128\n",
      "   4.683968    5.3352394   6.013945    6.1807065   4.7534194   0.42384252\n",
      "   4.464132    3.7691517   2.4586282   4.402055    2.8571541   6.6906343\n",
      "   6.184098    5.3716106   3.4062757   2.3102      5.7293816   7.667874\n",
      "   7.5095973   6.441735    6.0797663   5.2714663   6.170042    6.650445\n",
      "   6.615059    6.1368814   4.9097047   6.3077517   8.295402    9.065367\n",
      "   3.3056793   5.231379    5.458664    6.299503    9.405521    5.9441566\n",
      "   6.0257545   4.6988235   6.45687     1.0376215   3.9554236   4.897795\n",
      "   5.080752    6.340201    5.392864    4.9949746   6.0304255   4.9894056 ]] \n",
      " [[ 8.826611  10.8986225]]\n",
      "Epoch 30, train loss 1.7462, acc 0.7679; test loss 2.8128, acc 0.6933\n",
      "[[ 6.6698384   0.46647358  7.0157633   4.202422    3.7225628   4.0898542\n",
      "   4.776081    4.4112463   5.3011823   3.0181975   3.2067454   4.9277315\n",
      "   4.1976566   4.3764257   3.2019954   3.3986807   4.741868    4.965548\n",
      "   5.1971145   6.3985004   4.5097923   3.3966827   5.241953    4.0991416\n",
      "   3.0618658   3.175908    2.221515    3.5447507   5.243666    4.2072406\n",
      "   3.2370052   4.977275    5.868368    7.3008537  11.257145    7.9551563\n",
      "   6.3571796   4.899253    4.08        5.913779    4.8362446   4.3198466\n",
      "   4.562767    6.3829393   5.385753    7.141672    3.1171536   3.1380332\n",
      "   4.799657    5.085899    5.9290237   5.820144    4.098312    0.1231162\n",
      "   4.4140606   3.5592105   1.6754986   4.3433924   2.8520691   6.5845838\n",
      "   6.1514378   5.1367497   3.420247    2.092103    5.2482862   7.423315\n",
      "   7.40702     6.1312737   6.1211433   4.896702    5.967798    6.39975\n",
      "   6.6534142   6.090456    4.8176804   6.147693    7.4779315   8.938635\n",
      "   3.268331    5.103278    5.1487846   6.2426863   9.2339735   5.5593805\n",
      "   5.630828    4.7212486   6.358257    0.4757588   3.8438287   4.8536243\n",
      "   4.9632196   6.236391    5.241133    5.000501    6.262625    5.017597  ]] \n",
      " [[ 7.4553313 12.492028 ]]\n",
      "Epoch 40, train loss 1.0790, acc 0.8255; test loss 2.2871, acc 0.7182\n",
      "[[ 6.452959    0.3503653   6.4186673   4.299379    3.4574852   4.0031095\n",
      "   4.790795    4.493787    5.2400756   2.9775546   3.2469845   4.8575706\n",
      "   4.011983    4.3533163   3.1306467   3.1780884   4.6555395   4.763855\n",
      "   5.0876174   6.301482    4.40117     3.4274397   5.197569    4.054686\n",
      "   3.0818949   3.2014797   2.2122276   3.351092    5.0131583   4.0521846\n",
      "   2.9841454   4.9749312   5.933841    7.224135   11.068694    7.8660107\n",
      "   6.388287    4.5385685   4.147415    5.7519326   4.876343    4.261216\n",
      "   4.508161    6.1793804   5.1460824   7.1156487   2.9874053   3.2350063\n",
      "   4.867665    4.945802    5.7508793   5.6411614   3.6546133   0.1706694\n",
      "   4.5121307   3.405474    1.2090262   4.2931447   2.8559363   6.5064807\n",
      "   6.036928    4.9507594   3.3552697   2.0372663   5.1789007   7.3782153\n",
      "   7.316033    5.9089165   6.1523247   4.6195226   5.9061203   6.1915994\n",
      "   6.451048    6.1347766   4.8107452   6.0830493   6.769668    8.826416\n",
      "   3.239306    5.0403805   4.906775    6.2004848   8.911582    5.2816916\n",
      "   5.307991    4.72002     6.206508    0.24966282  3.6718638   4.8076563\n",
      "   4.8568687   6.1099014   5.1515713   5.054351    6.2927437   5.028734  ]] \n",
      " [[ 6.244018 13.571402]]\n",
      "Epoch 50, train loss 0.6050, acc 0.8790; test loss 1.8933, acc 0.7400\n",
      "[[ 6.3439007   0.31719872  6.381678    4.29902     3.2347336   3.9516392\n",
      "   4.8125596   4.5433426   5.225874    2.8865175   3.2642732   4.847453\n",
      "   3.8832822   4.336411    3.1163635   2.9585986   4.5735536   4.6123714\n",
      "   4.9926896   6.2871213   4.371728    3.46073     5.1348214   4.0414286\n",
      "   3.118778    3.2474284   2.2139695   3.2106445   4.855411    4.1735907\n",
      "   2.8344066   4.959795    5.935699    7.2448397  10.963124    7.7692914\n",
      "   6.2919326   4.266313    4.23088     5.736906    4.8744607   4.2229433\n",
      "   4.509064    6.0217304   4.982814    7.1027536   2.9355302   3.3005216\n",
      "   5.010205    4.925763    5.64568     5.4648185   3.346057    0.14802311\n",
      "   4.566099    3.3290029   0.9024643   4.299254    2.902965    6.4389625\n",
      "   6.0125055   4.8119364   3.2261927   2.0270226   5.0955896   7.396607\n",
      "   7.2687683   5.7875276   6.1503816   4.4999447   5.8487344   6.013028\n",
      "   6.1832314   6.197838    4.8702435   6.1581273   6.3832774   8.776702\n",
      "   3.200512    4.9838586   4.713559    6.2028875   8.683916    5.020138\n",
      "   4.9800606   4.694321    6.09812     0.13525769  3.41261     4.8316164\n",
      "   4.7694283   6.0166373   5.0887775   4.9465113   6.3694563   5.024344  ]] \n",
      " [[ 5.6740484 14.683101 ]]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = train_data['data'], [1 if i == 'pos' else 0 for i in train_data['label']]\n",
    "test_x, test_y = test_data['data'], [1 if i == 'pos' else 0 for i in test_data['label']]\n",
    "cnn_model = TextCNN(seq_length=train_x.shape[1], class_num=2, ebd_weights=ebd_weights, filter_num=32)\n",
    "batch_size = 1000\n",
    "epoch_max = 50\n",
    "\n",
    "\n",
    "config = use_gpu_polite()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch_now = sess.run(cnn_model.global_step)\n",
    "    \n",
    "    while epoch_now < epoch_max:\n",
    "        # 在训练集上按batch训练完所有，算作一个epoch\n",
    "        batch_num = train_x.shape[0] // batch_size\n",
    "        for i in range(batch_num+1):\n",
    "            s_i = i * batch_size\n",
    "            e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "            if s_i >= e_i:\n",
    "                continue\n",
    "            in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "            feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "            sess.run(cnn_model.train_op, feed_dict)\n",
    "\n",
    "        epoch_now = sess.run(cnn_model.global_step_op)  # 跑完了一个epoch，epoch+1\n",
    "        \n",
    "        if epoch_now % 10 == 0:  # 每10轮观察一下训练集测试集loss 和 acc\n",
    "            # 训练集总的损失和acc也要分步测，否则内存不够\n",
    "            batch_num = train_x.shape[0] // batch_size\n",
    "            train_total_loss = 0\n",
    "            train_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                train_loss_one, train_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                train_total_loss += train_loss_one\n",
    "                train_total_correct += train_correct_one\n",
    "            train_loss = train_total_loss / train_x.shape[0]\n",
    "            train_acc = train_total_correct / train_x.shape[0]\n",
    "\n",
    "            # 测试集的损失和acc\n",
    "            batch_num = test_x.shape[0] // batch_size\n",
    "            test_total_loss = 0\n",
    "            test_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, test_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = test_x[s_i: e_i, :], test_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                test_loss_one, test_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                test_total_loss += test_loss_one\n",
    "                test_total_correct += test_correct_one\n",
    "            test_loss = test_total_loss / test_x.shape[0]\n",
    "            test_acc = test_total_correct / test_x.shape[0]\n",
    "\n",
    "            # 查看一组 中间参数\n",
    "            flatten_param, logits = sess.run([cnn_model.pool_flatten, cnn_model.logits], \n",
    "                                             {cnn_model.seq_input: train_x[0:1, :]})\n",
    "            print('Epoch %d, train loss %.4f, acc %.4f; test loss %.4f, acc %.4f' % \n",
    "                  (epoch_now, train_loss, train_acc, test_loss, test_acc))\n",
    "            print(flatten_param, '\\n', logits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

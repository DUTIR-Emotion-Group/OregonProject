{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir923/wujiaming/anaconda3/envs/tf2/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from load_data import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "def use_gpu_polite(using_rate=0.6):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = using_rate\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickle data from ../data/imdb_data_3col.pkl\n",
      "Original 134957 words in vocabulary.\n",
      "After truncated low frequent word:\n",
      "words num: 40000/134957; words freq: 0.981\n",
      "Words exit in w2v file: 39210/40004, rate: 98.015198%\n",
      "Shape of weight matrix: (40006, 50)\n",
      "Train data shape: (25000, 500) label length: 25000\n",
      "Test data shape: (25000, 500) label length: 25000\n",
      "dict_keys(['data', 'data_len', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# 导入数据，train data 是一个字典， ebd embdding 是词向量矩阵，作为embedding层的初始参数\n",
    "train_data, test_data, ebd_weights = load_imdb_data()\n",
    "print(train_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TextCNN模型，使用layer借口实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TextCNN_layer():\n",
    "    def __init__(self, seq_length, class_num, ebd_weights, filter_num, filter_sizes = [2, 3, 4]):\n",
    "        seed_num = 7\n",
    "\n",
    "        self.seq_input = tf.placeholder(dtype=tf.int32, shape=[None, seq_length],\n",
    "                                        name='sequence_input')\n",
    "        self.sparse_label_input = tf.placeholder(dtype=tf.int32, shape=[None],\n",
    "                                                 name='sparse_label')\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.global_step_op = tf.assign(self.global_step, self.global_step+1)\n",
    "\n",
    "        embedding_dim = ebd_weights.shape[1]\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.W = tf.Variable(initial_value=ebd_weights, name='W')\n",
    "            self.embedding_layer = tf.nn.embedding_lookup(self.W, self.seq_input)\n",
    "            # batch * seq_len * emb_dim * in_channel(1)\n",
    "            self.embedding_layer_expand = tf.expand_dims(self.embedding_layer, axis=-1)\n",
    "\n",
    "        pool_layers = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope('conv-maxpool-%s' % i):\n",
    "                conv = tf.layers.conv2d(self.embedding_layer_expand, filter_num, [filter_size, embedding_dim],\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.1, seed=seed_num),\n",
    "                                bias_initializer=tf.constant_initializer(0.1),\n",
    "                                activation='relu', padding='VALID')\n",
    "                pool = tf.layers.max_pooling2d(conv, pool_size=[seq_length-filter_size+1, 1], \n",
    "                                               strides=[1, 1], padding='VALID')\n",
    "                pool_layers.append(pool)\n",
    "        \n",
    "        all_dim = len(filter_sizes) * filter_num\n",
    "        self.pool_flatten = tf.reshape(tf.concat(pool_layers, -1), shape=[-1, all_dim])\n",
    "        \n",
    "        with tf.name_scope('output'):\n",
    "            self.logits = tf.layers.dense(self.pool_flatten, class_num, \n",
    "                                          kernel_initializer=tf.truncated_normal_initializer(seed=seed_num), \n",
    "                                          bias_initializer=tf.constant_initializer(0.1))\n",
    "            self.prediction = tf.argmax(self.logits, axis=-1, output_type=tf.int32, name='prediction')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.sparse_label_input, logits=self.logits)\n",
    "            self.loss_sum = tf.reduce_sum(losses)\n",
    "            self.loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predction = tf.equal(self.prediction, self.sparse_label_input)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predction, tf.float16), name='accuracy')\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predction, tf.float16), name='accuracy')\n",
    "        \n",
    "        self.learning_rate = 1e-3\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        gs_vs = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(gs_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train loss 4.5977, acc 0.6211; test loss 5.1814, acc 0.5986\n",
      "Epoch 20, train loss 2.6009, acc 0.7113; test loss 3.4573, acc 0.6628\n",
      "Epoch 30, train loss 1.6857, acc 0.7708; test loss 2.7409, acc 0.6968\n",
      "Epoch 40, train loss 1.0510, acc 0.8230; test loss 2.2396, acc 0.7228\n",
      "Epoch 50, train loss 0.6041, acc 0.8755; test loss 1.8718, acc 0.7436\n",
      "Epoch 60, train loss 0.2983, acc 0.9262; test loss 1.5827, acc 0.7633\n",
      "Epoch 70, train loss 0.1129, acc 0.9696; test loss 1.3508, acc 0.7822\n",
      "Epoch 80, train loss 0.0355, acc 0.9928; test loss 1.1992, acc 0.8007\n",
      "Epoch 90, train loss 0.0164, acc 0.9962; test loss 1.1789, acc 0.8035\n",
      "Epoch 100, train loss 0.0045, acc 0.9991; test loss 1.1691, acc 0.8008\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = train_data['data'], [1 if i == 'pos' else 0 for i in train_data['label']]\n",
    "test_x, test_y = test_data['data'], [1 if i == 'pos' else 0 for i in test_data['label']]\n",
    "cnn_model = TextCNN_layer(seq_length=train_x.shape[1], class_num=2, ebd_weights=ebd_weights, filter_num=32)\n",
    "batch_size = 1000\n",
    "epoch_max = 100\n",
    "\n",
    "\n",
    "config = use_gpu_polite()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch_now = sess.run(cnn_model.global_step)\n",
    "    \n",
    "    while epoch_now < epoch_max:\n",
    "        # 在训练集上按batch训练完所有，算作一个epoch\n",
    "        batch_num = train_x.shape[0] // batch_size\n",
    "        for i in range(batch_num+1):\n",
    "            s_i = i * batch_size\n",
    "            e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "            if s_i >= e_i:\n",
    "                continue\n",
    "            in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "            feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "            sess.run(cnn_model.train_op, feed_dict)\n",
    "\n",
    "        epoch_now = sess.run(cnn_model.global_step_op)  # 跑完了一个epoch，epoch+1\n",
    "        \n",
    "        if epoch_now % 10 == 0:  # 每10轮观察一下训练集测试集loss 和 acc\n",
    "            # 训练集总的损失和acc也要分步测，否则内存不够\n",
    "            batch_num = train_x.shape[0] // batch_size\n",
    "            train_total_loss = 0\n",
    "            train_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                train_loss_one, train_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                train_total_loss += train_loss_one\n",
    "                train_total_correct += train_correct_one\n",
    "            train_loss = train_total_loss / train_x.shape[0]\n",
    "            train_acc = train_total_correct / train_x.shape[0]\n",
    "\n",
    "            # 测试集的损失和acc\n",
    "            batch_num = test_x.shape[0] // batch_size\n",
    "            test_total_loss = 0\n",
    "            test_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, test_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = test_x[s_i: e_i, :], test_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                test_loss_one, test_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                test_total_loss += test_loss_one\n",
    "                test_total_correct += test_correct_one\n",
    "            test_loss = test_total_loss / test_x.shape[0]\n",
    "            test_acc = test_total_correct / test_x.shape[0]\n",
    "\n",
    "            print('Epoch %d, train loss %.4f, acc %.4f; test loss %.4f, acc %.4f' % \n",
    "                  (epoch_now, train_loss, train_acc, test_loss, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**两个4-开头的文件都是实现TextCNN，这个使用的是lyaers接口，另一个扎扎实实地写conv和dense的每个参数。**  \n",
    "**两种方法使用的参数初始化方法完全一样，随机种子也一样，都设置成7。期待训练结果在loss和acc的数值上完全一致。**  \n",
    "**但是，还是存在细微差异，虽然总体上差别不大（几乎小数点后三位）。可能是因为conv有32个kernel，那里随机初始化两种方法有偏差。**  \n",
    "**可以看到，layers接口集成度高，方便很多。事实上，可能底层方法和就和自己写的差不多。所以之后尽量都用layers接口去实现模型。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

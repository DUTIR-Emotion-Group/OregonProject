{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutir923/wujiaming/anaconda3/envs/tf2/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from load_data import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' #使用 GPU 0\n",
    "def use_gpu_polite(using_rate=0.6):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = using_rate\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickle data from ../data/imdb_data_3col.pkl\n",
      "Original 134957 words in vocabulary.\n",
      "After truncated low frequent word:\n",
      "words num: 40000/134957; words freq: 0.981\n",
      "Words exit in w2v file: 39210/40004, rate: 98.015198%\n",
      "Shape of weight matrix: (40006, 50)\n",
      "Train data shape: (25000, 500) label length: 25000\n",
      "Test data shape: (25000, 500) label length: 25000\n",
      "dict_keys(['data', 'data_len', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# 导入数据，train data 是一个字典， ebd embdding 是词向量矩阵，作为embedding层的初始参数\n",
    "train_data, test_data, ebd_weights = load_imdb_data()\n",
    "print(train_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN模型，使用layer借口实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN_layer():\n",
    "    def __init__(self, seq_length, class_num, ebd_weights, filter_num, filter_sizes = [2, 3, 4]):\n",
    "        seed_num = 7\n",
    "\n",
    "        self.seq_input = tf.placeholder(dtype=tf.int32, shape=[None, seq_length],\n",
    "                                        name='sequence_input')\n",
    "        self.sparse_label_input = tf.placeholder(dtype=tf.int32, shape=[None],\n",
    "                                                 name='sparse_label')\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.global_step_op = tf.assign(self.global_step, self.global_step+1)\n",
    "\n",
    "        embedding_dim = ebd_weights.shape[1]\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.W = tf.Variable(initial_value=ebd_weights, name='W')\n",
    "            self.embedding_layer = tf.nn.embedding_lookup(self.W, self.seq_input)\n",
    "            # batch * seq_len * emb_dim * in_channel(1)\n",
    "            self.embedding_layer_expand = tf.expand_dims(self.embedding_layer, axis=-1)\n",
    "\n",
    "        pool_layers = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope('conv-maxpool-%s' % i):\n",
    "                conv = tf.layers.conv2d(self.embedding_layer_expand, filter_num, [filter_size, embedding_dim],\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.1, seed=seed_num),\n",
    "                                bias_initializer=tf.constant_initializer(0.1),\n",
    "                                activation='relu', padding='VALID')\n",
    "                pool = tf.layers.max_pooling2d(conv, pool_size=[seq_length-filter_size+1, 1], \n",
    "                                               strides=[1, 1], padding='VALID')\n",
    "                pool_layers.append(pool)\n",
    "        \n",
    "        all_dim = len(filter_sizes) * filter_num\n",
    "        self.pool_flatten = tf.reshape(tf.concat(pool_layers, -1), shape=[-1, all_dim])\n",
    "        \n",
    "#         with tf.name_scope('output'):\n",
    "#             self.logits = tf.layers.dense(self.pool_flatten, class_num, \n",
    "#                                           kernel_initializer=tf.truncated_normal_initializer(seed=seed_num), \n",
    "#                                           bias_initializer=tf.constant_initializer(0.1))\n",
    "#             self.prediction = tf.argmax(self.logits, axis=-1, output_type=tf.int32, name='prediction')\n",
    "\n",
    "        with tf.name_scope('output'):\n",
    "            W = tf.Variable(tf.truncated_normal([all_dim, class_num], seed=seed_num, name='W'))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[class_num]), name='b')\n",
    "            self.logits = tf.nn.xw_plus_b(self.pool_flatten, W, b, name='logits')\n",
    "            self.prediction = tf.argmax(self.logits, axis=-1, output_type=tf.int32, name='prediction')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.sparse_label_input, logits=self.logits)\n",
    "            self.loss_sum = tf.reduce_sum(losses)\n",
    "            self.loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predction = tf.equal(self.prediction, self.sparse_label_input)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predction, tf.float16), name='accuracy')\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predction, tf.float16), name='accuracy')\n",
    "        \n",
    "        self.learning_rate = 1e-3\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        gs_vs = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(gs_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train loss 4.5887, acc 0.6209; test loss 5.1708, acc 0.5989\n",
      "[[5.098266   0.6267031  3.973796   5.505822   2.2480958  4.749883\n",
      "  5.784947   3.8512633  4.734154   4.691539   4.447296   4.1583285\n",
      "  5.139508   3.5924542  4.315198   5.1952605  3.5926905  3.703297\n",
      "  4.0715146  4.777809   3.2233436  2.4266162  6.819762   6.15767\n",
      "  3.3594055  3.9477615  3.7517774  4.710288   6.134476   4.665935\n",
      "  4.40662    3.2140205  5.3673306  5.9726353  5.72242    7.0440216\n",
      "  4.2133174  5.263072   5.6293364  5.663528   4.213584   5.853416\n",
      "  7.70406    4.1801705  7.8236217  4.993251   4.7438684  7.057441\n",
      "  4.4783754  5.1031847  4.5588865  6.079002   5.352418   0.51727957\n",
      "  8.656329   6.2806416  1.7194276  5.798382   2.59123    6.4119115\n",
      "  5.9181786  4.2614393  3.4503758  3.3350475  5.5210953  5.8865037\n",
      "  5.852014   6.280078   5.3702617  6.1314893  5.965096   4.957735\n",
      "  6.4592805  5.5918875  6.2671504  4.5609756  7.2407146  5.4812913\n",
      "  6.6172347  6.0109396  5.342335   4.9947124  4.027345   5.875778\n",
      "  5.379907   4.4371696  7.9950733  3.836801   6.45862    5.288876\n",
      "  6.068714   5.433056   6.2122593  4.6483364  6.0451922  6.6902194 ]] \n",
      " [[2.1574857 2.0837326]]\n",
      "Epoch 20, train loss 2.6062, acc 0.7114; test loss 3.4620, acc 0.6623\n",
      "[[5.0362387  0.19801545 4.166254   5.645503   2.1786363  4.341589\n",
      "  5.7246795  3.870985   4.649233   4.689161   4.4093604  4.1537423\n",
      "  4.925273   3.4793952  3.7729144  4.6185513  3.8852844  3.6047602\n",
      "  4.008902   4.7508564  3.120088   2.5783763  6.1200347  6.095701\n",
      "  3.2462995  3.9332817  3.5993598  3.8454936  6.0459194  4.4793706\n",
      "  4.1733093  3.196469   5.1539364  6.36797    5.8512473  6.84206\n",
      "  4.0303574  4.2603183  5.6542416  5.494475   4.031395   5.808306\n",
      "  7.414172   4.209555   7.4844656  4.928098   4.65112    7.16879\n",
      "  4.1454744  4.9913774  4.487235   5.8978844  5.178958   0.1836602\n",
      "  8.560258   6.087008   1.1873138  5.6187215  2.325016   6.2704935\n",
      "  5.854587   4.006222   3.3817737  2.8175013  4.898459   5.6668034\n",
      "  5.5912414  5.7498693  5.542494   5.4933777  6.2311926  4.849987\n",
      "  6.4571776  5.428884   5.544071   4.00728    6.5388227  5.4590015\n",
      "  6.659797   5.874529   5.7255583  5.1338167  3.687825   5.787207\n",
      "  4.997752   4.4673376  7.8396964  2.1554189  6.8275003  5.403716\n",
      "  5.6347284  5.353688   5.9332576  4.548898   5.6801977  6.6473923 ]] \n",
      " [[1.6741112 3.1136525]]\n",
      "Epoch 30, train loss 1.6805, acc 0.7712; test loss 2.7324, acc 0.6970\n",
      "[[5.016112   0.09273866 4.501522   5.6328735  2.04237    4.144498\n",
      "  5.305896   3.7844646  4.704204   4.686191   4.4292455  4.3097806\n",
      "  4.615505   3.38523    3.4430718  4.299903   4.020494   3.5773697\n",
      "  3.9552362  4.795371   3.0668724  2.5690422  6.062595   6.1404305\n",
      "  3.1780798  3.891404   3.5096817  3.4780407  5.969949   4.244549\n",
      "  3.947991   3.228486   5.2544117  6.4531374  5.8023787  6.672438\n",
      "  3.9225595  3.5435667  5.4143815  5.3274717  4.0424857  5.7448163\n",
      "  6.9132147  4.2419124  7.201419   4.780577   4.905217   7.158456\n",
      "  3.8814738  5.0064425  4.336192   5.907568   5.177832   0.1139906\n",
      "  8.462687   5.8699636  0.9612174  5.388182   2.0882862  6.110261\n",
      "  5.651369   3.8505192  3.394396   2.464963   4.473951   5.846192\n",
      "  5.4017315  5.5196843  5.591327   5.016323   6.38291    5.1700935\n",
      "  6.320998   5.23571    5.304131   3.6794326  6.070609   5.394554\n",
      "  6.5752206  5.6880765  5.9360576  5.2018547  3.7413628  5.9196553\n",
      "  4.7584677  4.3385615  7.8406935  1.3387026  6.757745   5.330963\n",
      "  5.4325805  5.242789   5.8687477  4.5527353  5.514936   6.7712846 ]] \n",
      " [[1.8462338 3.858971 ]]\n",
      "Epoch 40, train loss 1.0569, acc 0.8219; test loss 2.2429, acc 0.7222\n",
      "[[5.013577   0.11632124 4.6521807  5.5033307  1.9192209  3.9285026\n",
      "  4.906681   3.7335348  4.681615   4.660758   4.401728   4.4503746\n",
      "  4.325367   3.2514446  3.4327924  4.0366545  4.265534   3.5351682\n",
      "  3.9442291  4.849338   2.9715648  2.5341225  6.165883   6.223826\n",
      "  3.125138   3.8516552  3.4257615  3.248164   5.9209676  4.085754\n",
      "  3.7242582  3.2631402  5.27009    6.3431597  5.6833534  6.7052546\n",
      "  3.8045988  3.3163152  5.2614202  5.1540427  4.062528   5.728109\n",
      "  6.650795   4.3500824  6.8862343  4.641606   4.952965   7.135495\n",
      "  3.6628172  4.9547315  4.1166315  5.92768    5.1977854  0.17273566\n",
      "  8.362746   5.7200375  0.7671286  5.507053   2.011397   5.98478\n",
      "  5.572274   3.6387494  3.387463   2.3250275  3.971365   5.9496684\n",
      "  5.2970715  5.731473   5.6037474  4.694527   6.29139    5.323832\n",
      "  6.2856784  5.202038   5.15908    3.4226217  5.652772   5.237019\n",
      "  6.429028   5.508126   6.0408773  5.2021294  3.8784294  5.9817286\n",
      "  4.652418   4.308339   7.8871202  0.7820953  6.6738486  5.230804\n",
      "  5.293616   5.1971273  5.8157606  4.58819    5.420378   6.903554  ]] \n",
      " [[1.9665669 5.5652366]]\n",
      "Epoch 50, train loss 0.6057, acc 0.8746; test loss 1.8705, acc 0.7450\n",
      "[[4.950242   0.18762991 4.738632   5.3630505  1.84901    3.8303723\n",
      "  4.5785007  3.717008   4.6528234  4.592286   4.3563447  4.59693\n",
      "  4.1118345  3.143179   3.6451268  3.9109638  4.3431582  3.4989023\n",
      "  3.9121494  4.883363   2.899912   2.4973598  6.1918488  6.3353615\n",
      "  3.096661   3.8375814  3.344079   3.1550338  5.867676   3.928638\n",
      "  3.592138   3.2484868  5.1550946  6.209063   5.5951786  6.7666254\n",
      "  3.746469   3.2354014  5.254299   5.049071   4.1404757  5.6826277\n",
      "  6.6563506  4.4747777  6.547628   4.4867077  4.8694606  7.093491\n",
      "  3.5124547  4.912216   4.1491446  5.950279   5.2560663  0.22459403\n",
      "  8.167877   5.615115   0.6341869  5.712138   1.9965545  5.823379\n",
      "  5.4841156  3.5386786  3.41713    2.3080683  3.7741423  5.9613028\n",
      "  5.2985625  5.733398   5.5695677  4.5841775  6.179399   5.4860196\n",
      "  6.1690946  5.2078147  5.201507   3.284959   5.4460764  5.0275483\n",
      "  6.3292103  5.3946767  6.1105146  5.2133174  4.046075   6.012529\n",
      "  4.5719986  4.367323   7.905689   0.42062205 6.6023073  5.125855\n",
      "  5.233193   5.141142   5.847269   4.6497154  5.3511925  6.923817  ]] \n",
      " [[1.5204959 7.174155 ]]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = train_data['data'], [1 if i == 'pos' else 0 for i in train_data['label']]\n",
    "test_x, test_y = test_data['data'], [1 if i == 'pos' else 0 for i in test_data['label']]\n",
    "cnn_model = TextCNN_layer(seq_length=train_x.shape[1], class_num=2, ebd_weights=ebd_weights, filter_num=32)\n",
    "batch_size = 1000\n",
    "epoch_max = 50\n",
    "\n",
    "\n",
    "config = use_gpu_polite()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch_now = sess.run(cnn_model.global_step)\n",
    "    \n",
    "    while epoch_now < epoch_max:\n",
    "        # 在训练集上按batch训练完所有，算作一个epoch\n",
    "        batch_num = train_x.shape[0] // batch_size\n",
    "        for i in range(batch_num+1):\n",
    "            s_i = i * batch_size\n",
    "            e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "            if s_i >= e_i:\n",
    "                continue\n",
    "            in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "            feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "            sess.run(cnn_model.train_op, feed_dict)\n",
    "\n",
    "        epoch_now = sess.run(cnn_model.global_step_op)  # 跑完了一个epoch，epoch+1\n",
    "        \n",
    "        if epoch_now % 10 == 0:  # 每10轮观察一下训练集测试集loss 和 acc\n",
    "            # 训练集总的损失和acc也要分步测，否则内存不够\n",
    "            batch_num = train_x.shape[0] // batch_size\n",
    "            train_total_loss = 0\n",
    "            train_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                train_loss_one, train_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                train_total_loss += train_loss_one\n",
    "                train_total_correct += train_correct_one\n",
    "            train_loss = train_total_loss / train_x.shape[0]\n",
    "            train_acc = train_total_correct / train_x.shape[0]\n",
    "\n",
    "            # 测试集的损失和acc\n",
    "            batch_num = test_x.shape[0] // batch_size\n",
    "            test_total_loss = 0\n",
    "            test_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, test_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = test_x[s_i: e_i, :], test_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                test_loss_one, test_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                test_total_loss += test_loss_one\n",
    "                test_total_correct += test_correct_one\n",
    "            test_loss = test_total_loss / test_x.shape[0]\n",
    "            test_acc = test_total_correct / test_x.shape[0]\n",
    "\n",
    "            flatten_param, logits = sess.run([cnn_model.pool_flatten, cnn_model.logits], \n",
    "                                             {cnn_model.seq_input: train_x[0:1, :]})\n",
    "            print('Epoch %d, train loss %.4f, acc %.4f; test loss %.4f, acc %.4f' % \n",
    "                  (epoch_now, train_loss, train_acc, test_loss, test_acc))\n",
    "            print(flatten_param, '\\n', logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**两个4-开头的文件都是实现TextCNN，这个使用的是lyaers接口，另一个扎扎实实地写conv和dense的每个参数。**  \n",
    "**两种方法使用的参数初始化方法完全一样，随机种子也一样，都设置成7。期待训练结果在loss和acc的数值上完全一致。**  \n",
    "**但是，还是存在细微差异，虽然总体上差别不大（几乎小数点后三位）。**  \n",
    "**同时还打出了pooling之后的tensor，以及logits。发现pooling之后就有细微差距了，应该就是conv层产生了不同。**  \n",
    "**猜测一：可能是因为conv有32个kernel，那里随机初始化两种方法有偏差。**  \n",
    "**猜测二：还存在其它参数初始化存在随机，或者训练过程中有随机。因为两次运行同一个文件，flatten层输出也不一样。**  \n",
    "**可以看到，layers接口集成度高，方便很多。事实上，可能底层方法和就和自己写的差不多。所以之后尽量都用layers接口去实现模型。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

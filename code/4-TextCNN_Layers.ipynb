{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' #使用 GPU 0\n",
    "def use_gpu_polite(using_rate=0.6):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = using_rate\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pickle data from ../data/imdb_data_3col.pkl\n",
      "Original 134957 words in vocabulary.\n",
      "After truncated low frequent word:\n",
      "words num: 40000/134957; words freq: 0.981\n",
      "Words exit in w2v file: 39210/40004, rate: 98.015198%\n",
      "Shape of weight matrix: (40006, 50)\n",
      "Train data shape: (25000, 500) label length: 25000\n",
      "Test data shape: (25000, 500) label length: 25000\n",
      "dict_keys(['data', 'data_len', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# 导入数据，train data 是一个字典， ebd embdding 是词向量矩阵，作为embedding层的初始参数\n",
    "train_data, test_data, ebd_weights = load_imdb_data()\n",
    "print(train_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN模型，使用layer借口实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN_layer():\n",
    "    def __init__(self, seq_length, class_num, ebd_weights, filter_num, filter_sizes = [2, 3, 4]):\n",
    "        seed_num = 7\n",
    "\n",
    "        self.seq_input = tf.placeholder(dtype=tf.int32, shape=[None, seq_length],\n",
    "                                        name='sequence_input')\n",
    "        self.sparse_label_input = tf.placeholder(dtype=tf.int32, shape=[None],\n",
    "                                                 name='sparse_label')\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.global_step_op = tf.assign(self.global_step, self.global_step+1)\n",
    "\n",
    "        embedding_dim = ebd_weights.shape[1]\n",
    "        with tf.name_scope('embedding'):\n",
    "            self.W = tf.Variable(initial_value=ebd_weights, name='W')\n",
    "            self.embedding_layer = tf.nn.embedding_lookup(self.W, self.seq_input)\n",
    "            # batch * seq_len * emb_dim * in_channel(1)\n",
    "            self.embedding_layer_expand = tf.expand_dims(self.embedding_layer, axis=-1)\n",
    "\n",
    "        pool_layers = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope('conv-maxpool-%s' % i):\n",
    "                conv = tf.layers.conv2d(self.embedding_layer_expand, filter_num, [filter_size, embedding_dim],\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.1, seed=seed_num),\n",
    "                                bias_initializer=tf.constant_initializer(0.1),\n",
    "                                activation='relu', padding='VALID')\n",
    "                pool = tf.layers.max_pooling2d(conv, pool_size=[seq_length-filter_size+1, 1], \n",
    "                                               strides=[1, 1], padding='VALID')\n",
    "                pool_layers.append(pool)\n",
    "        \n",
    "        all_dim = len(filter_sizes) * filter_num\n",
    "        self.pool_flatten = tf.reshape(tf.concat(pool_layers, -1), shape=[-1, all_dim])\n",
    "        \n",
    "#         with tf.name_scope('output'):\n",
    "#             self.logits = tf.layers.dense(self.pool_flatten, class_num, \n",
    "#                                           kernel_initializer=tf.truncated_normal_initializer(seed=seed_num), \n",
    "#                                           bias_initializer=tf.constant_initializer(0.1))\n",
    "#             self.prediction = tf.argmax(self.logits, axis=-1, output_type=tf.int32, name='prediction')\n",
    "\n",
    "        with tf.name_scope('output'):\n",
    "            W = tf.Variable(tf.truncated_normal([all_dim, class_num], seed=seed_num, name='W'))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[class_num]), name='b')\n",
    "            self.logits = tf.nn.xw_plus_b(self.pool_flatten, W, b, name='logits')\n",
    "            self.prediction = tf.argmax(self.logits, axis=-1, output_type=tf.int32, name='prediction')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.sparse_label_input, logits=self.logits)\n",
    "            self.loss_sum = tf.reduce_sum(losses)\n",
    "            self.loss = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predction = tf.equal(self.prediction, self.sparse_label_input)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predction, tf.float16), name='accuracy')\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predction, tf.float16), name='accuracy')\n",
    "        \n",
    "        self.learning_rate = 1e-3\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        gs_vs = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(gs_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train loss 4.9552, acc 0.6108; test loss 5.5408, acc 0.5916\n",
      "[[ 7.1507683  0.9235705  8.771172   4.0647635  4.441228   4.078831\n",
      "   4.686659   4.563436   5.448373   3.3299072  3.1972177  5.536707\n",
      "   4.7202597  4.5665407  3.3574715  3.9295497  5.0851145  5.815064\n",
      "   5.3960667  6.717537   4.6561666  3.429185   5.158939   4.4089956\n",
      "   2.8463964  3.0861168  2.144047   4.4400115  6.0825334  4.400116\n",
      "   4.140744   5.149525   5.54589    8.144511  11.44268    8.524857\n",
      "   7.023779   5.759207   4.148216   6.2796974  5.3028364  4.8843536\n",
      "   4.455388   6.9233866  6.0895743  7.3255033  3.6607678  3.5709817\n",
      "   4.6960087  5.728938   6.2494655  6.6449275  5.6996465  1.3828605\n",
      "   4.6555014  3.9797611  3.461446   4.642683   2.8416858  6.8132925\n",
      "   6.197632   5.5107903  3.4199927  3.0365307  6.5887294  7.9431195\n",
      "   7.456849   6.758239   6.1292887  5.710578   6.460667   6.791635\n",
      "   6.988167   6.355074   4.7688093  6.770875   9.24423    9.064208\n",
      "   3.5329266  5.6233883  5.7817974  6.2300353  9.221376   6.2416363\n",
      "   6.663696   4.5837984  6.7200766  2.1029751  4.0574207  4.7880154\n",
      "   5.0745463  6.5036983  5.868791   5.315082   5.9787054  4.7977886]] \n",
      " [[11.94177   9.166911]]\n",
      "Epoch 20, train loss 2.8215, acc 0.7004; test loss 3.6905, acc 0.6556\n",
      "[[ 6.8058767  0.682637   7.9667106  4.057022   4.043301   4.0952783\n",
      "   4.7421927  4.3982553  5.4283204  3.071113   3.1132631  5.162528\n",
      "   4.49036    4.4543853  3.2535434  3.6930633  4.812409   5.3171234\n",
      "   5.2684555  6.524411   4.637555   3.2923727  5.2552347  4.220566\n",
      "   3.0282893  3.1397595  2.2912426  3.850227   5.6268888  4.2896457\n",
      "   3.549079   5.0524006  5.819707   7.6229377 11.445266   8.11057\n",
      "   6.4659824  5.4188247  4.0240827  6.091327   4.8907003  4.480201\n",
      "   4.564445   6.5027304  5.7391696  7.228443   3.3225405  3.2364917\n",
      "   4.697893   5.339584   6.011006   6.1851854  4.759748   0.4109675\n",
      "   4.4600697  3.7775657  2.498242   4.4000325  2.9055603  6.6980643\n",
      "   6.1947546  5.3823104  3.4251184  2.274392   5.732239   7.655014\n",
      "   7.4696913  6.4223504  6.092685   5.3670425  6.145508   6.598414\n",
      "   6.623401   6.173798   4.942243   6.3146853  8.287988   9.040498\n",
      "   3.317487   5.247971   5.464834   6.304265   9.393644   5.946129\n",
      "   6.072946   4.680631   6.468109   1.0412406  3.9950013  4.9259176\n",
      "   5.0453334  6.323977   5.3844666  5.0475626  6.0499372  4.955938 ]] \n",
      " [[ 8.719883 10.768715]]\n",
      "Epoch 30, train loss 1.7436, acc 0.7677; test loss 2.8152, acc 0.6930\n",
      "[[ 6.6277027   0.52692825  7.0237837   4.1710353   3.7249122   4.0895696\n",
      "   4.756025    4.4038863   5.323182    3.0192418   3.1834188   4.9420485\n",
      "   4.2005105   4.38995     3.202921    3.402773    4.7296686   4.965729\n",
      "   5.1825857   6.4067383   4.507216    3.4080446   5.212308    4.1686225\n",
      "   3.057402    3.1708214   2.3284795   3.5341382   5.249484    4.195404\n",
      "   3.2566483   4.9676504   5.862662    7.334225   11.256576    7.961018\n",
      "   6.3653436   4.90581     4.03634     5.9324007   4.822524    4.301584\n",
      "   4.561169    6.3685412   5.4134746   7.1478467   3.105695    3.1277752\n",
      "   4.8148837   5.103224    5.9158173   5.8327823   4.112288    0.13908142\n",
      "   4.4286113   3.562714    1.7125692   4.321131    2.897803    6.573832\n",
      "   6.1582055   5.1823263   3.4242456   2.1294281   5.242593    7.4112425\n",
      "   7.397319    6.091016    6.1380873   5.070623    5.9302325   6.3621473\n",
      "   6.679786    6.133153    4.8002143   6.156574    7.489283    8.904404\n",
      "   3.2733278   5.0923624   5.1293964   6.2355175   9.238206    5.5500937\n",
      "   5.7049904   4.676634    6.3610773   0.46793547  3.8769336   4.881437\n",
      "   4.922534    6.2109303   5.2217264   5.0635934   6.2900167   4.998473  ]] \n",
      " [[ 7.490148 12.293872]]\n",
      "Epoch 40, train loss 1.0708, acc 0.8265; test loss 2.2824, acc 0.7176\n",
      "[[ 6.449337    0.42656025  6.43406     4.2702055   3.4774988   3.9994802\n",
      "   4.777944    4.492763    5.2626066   2.984818    3.2099056   4.859688\n",
      "   4.0241933   4.3665915   3.1477046   3.167913    4.6505938   4.7618356\n",
      "   5.070694    6.3001084   4.4097424   3.4065633   5.1887693   4.1112533\n",
      "   3.0510075   3.194399    2.3186827   3.3435333   5.0117455   4.04386\n",
      "   2.994202    4.971848    5.9227757   7.282948   11.064148    7.8587775\n",
      "   6.3902607   4.5819936   4.100811    5.8134885   4.84702     4.247707\n",
      "   4.528241    6.1784286   5.1540003   7.1288204   2.9612396   3.2042158\n",
      "   4.906429    4.9492455   5.714003    5.6495614   3.6866443   0.18071382\n",
      "   4.5375123   3.411412    1.2356607   4.2683825   2.9318175   6.5124617\n",
      "   6.0463495   4.973304    3.3837318   2.0988793   5.1966715   7.3969197\n",
      "   7.363907    5.853495    6.1621776   4.853988    5.881024    6.1702633\n",
      "   6.4401126   6.138502    4.7854567   6.1242056   6.769551    8.782315\n",
      "   3.2281885   5.043812    4.8746734   6.1928344   8.930077    5.2698073\n",
      "   5.412004    4.6774306   6.222105    0.22550948  3.7141938   4.842649\n",
      "   4.8112836   6.0718503   5.1397467   5.138451    6.3210783   4.993807  ]] \n",
      " [[ 6.199834 13.332283]]\n",
      "Epoch 50, train loss 0.5986, acc 0.8801; test loss 1.8881, acc 0.7396\n",
      "[[ 6.337916    0.40981108  6.330917    4.304086    3.2435777   3.939303\n",
      "   4.802409    4.5457673   5.252262    2.898762    3.2204475   4.84404\n",
      "   3.8972204   4.336458    3.1090817   2.9268677   4.571533    4.626031\n",
      "   4.99224     6.282968    4.3933306   3.4083812   5.1344476   4.1188188\n",
      "   3.0915267   3.1927056   2.3285897   3.2197871   4.8311844   4.1626325\n",
      "   2.815552    4.967158    5.9511156   7.308415   10.957615    7.7536683\n",
      "   6.3058467   4.2876425   4.2048273   5.7463875   4.841693    4.225612\n",
      "   4.5383596   5.9978967   4.9900494   7.1026554   2.904499    3.2723246\n",
      "   5.0174017   4.9317703   5.5463357   5.48292     3.3477159   0.12795983\n",
      "   4.6107893   3.316282    0.93509233  4.267066    3.024991    6.44557\n",
      "   5.9914694   4.783921    3.2659328   2.0636494   5.089259    7.4159718\n",
      "   7.318694    5.7038083   6.1534615   4.7138896   5.867352    5.9941382\n",
      "   6.179625    6.2026      4.8301473   6.140466    6.356625    8.740635\n",
      "   3.2059665   4.9904227   4.7105827   6.1893835   8.730104    4.9892144\n",
      "   5.112278    4.671481    6.098844    0.13628224  3.4647884   4.853035\n",
      "   4.7335067   5.9954424   5.076558    5.034132    6.3951597   4.958125  ]] \n",
      " [[ 5.536484 14.462185]]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = train_data['data'], [1 if i == 'pos' else 0 for i in train_data['label']]\n",
    "test_x, test_y = test_data['data'], [1 if i == 'pos' else 0 for i in test_data['label']]\n",
    "cnn_model = TextCNN_layer(seq_length=train_x.shape[1], class_num=2, ebd_weights=ebd_weights, filter_num=32)\n",
    "batch_size = 1000\n",
    "epoch_max = 50\n",
    "\n",
    "\n",
    "config = use_gpu_polite()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch_now = sess.run(cnn_model.global_step)\n",
    "    \n",
    "    while epoch_now < epoch_max:\n",
    "        # 在训练集上按batch训练完所有，算作一个epoch\n",
    "        batch_num = train_x.shape[0] // batch_size\n",
    "        for i in range(batch_num+1):\n",
    "            s_i = i * batch_size\n",
    "            e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "            if s_i >= e_i:\n",
    "                continue\n",
    "            in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "            feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "            sess.run(cnn_model.train_op, feed_dict)\n",
    "\n",
    "        epoch_now = sess.run(cnn_model.global_step_op)  # 跑完了一个epoch，epoch+1\n",
    "        \n",
    "        if epoch_now % 10 == 0:  # 每10轮观察一下训练集测试集loss 和 acc\n",
    "            # 训练集总的损失和acc也要分步测，否则内存不够\n",
    "            batch_num = train_x.shape[0] // batch_size\n",
    "            train_total_loss = 0\n",
    "            train_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, train_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = train_x[s_i: e_i, :], train_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                train_loss_one, train_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                train_total_loss += train_loss_one\n",
    "                train_total_correct += train_correct_one\n",
    "            train_loss = train_total_loss / train_x.shape[0]\n",
    "            train_acc = train_total_correct / train_x.shape[0]\n",
    "\n",
    "            # 测试集的损失和acc\n",
    "            batch_num = test_x.shape[0] // batch_size\n",
    "            test_total_loss = 0\n",
    "            test_total_correct = 0\n",
    "            for i in range(batch_num+1):\n",
    "                s_i = i * batch_size\n",
    "                e_i = min((i+1)*batch_size, test_x.shape[0])\n",
    "                if s_i >= e_i:\n",
    "                    continue\n",
    "                in_x, in_y = test_x[s_i: e_i, :], test_y[s_i: e_i]\n",
    "                feed_dict = {cnn_model.seq_input: in_x, cnn_model.sparse_label_input: in_y}\n",
    "\n",
    "                test_loss_one, test_correct_one = sess.run([cnn_model.loss_sum, cnn_model.correct_num], feed_dict)\n",
    "                test_total_loss += test_loss_one\n",
    "                test_total_correct += test_correct_one\n",
    "            test_loss = test_total_loss / test_x.shape[0]\n",
    "            test_acc = test_total_correct / test_x.shape[0]\n",
    "\n",
    "            flatten_param, logits = sess.run([cnn_model.pool_flatten, cnn_model.logits], \n",
    "                                             {cnn_model.seq_input: train_x[0:1, :]})\n",
    "            print('Epoch %d, train loss %.4f, acc %.4f; test loss %.4f, acc %.4f' % \n",
    "                  (epoch_now, train_loss, train_acc, test_loss, test_acc))\n",
    "            print(flatten_param, '\\n', logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**两个4-开头的文件都是实现TextCNN，这个使用的是lyaers接口，另一个扎扎实实地写conv和dense的每个参数。**  \n",
    "**两种方法使用的参数初始化方法完全一样，随机种子也一样，都设置成7。期待训练结果在loss和acc的数值上完全一致。**  \n",
    "**但是，还是存在细微差异，虽然总体上差别不大（几乎小数点后三位）。**  \n",
    "**同时还打出了pooling之后的tensor，以及logits。发现pooling之后就有细微差距了，应该就是conv层产生了不同。**  \n",
    "**猜测一：可能是因为conv有32个kernel，那里随机初始化两种方法有偏差。**  \n",
    "**猜测二：还存在其它参数初始化存在随机，或者训练过程中有随机。因为两次运行同一个文件，flatten层输出也不一样。**  \n",
    "**可以看到，layers接口集成度高，方便很多。事实上，可能底层方法和就和自己写的差不多。所以之后尽量都用layers接口去实现模型。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
